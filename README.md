# Generating Sentences with Ngrams
This project uses the Natural Language Toolkit (nltk) library to create n-gram models of different texts in order to generate sentences that resemble the original. The sentences generated by different size n-grams were also compared to see how the size of the n-gram influenced the quality of the resulting sentences.

# Background
Natural Language Processing is an area that I've been interested in for a long time, so I recently decided I wanted to explore what you can actually do with it. Researching basic NLP pointed me towards using bigrams and trigrams to create a model of how the probability of a word appearing is inflenced by the words that come before it, and how this can be used to generate new text similar in style to the original. Since I'm a big reader, I was really intruiged by the idea of algorithmic text generation, and I wanted to see what I could do with it.

# Usage 
1. Install Python >3 and download the nltk package using the instructions [here](https://www.nltk.org/install.html)
2. In text-analysis.py, set `NGRAM_SIZE` to the n-gram size you would like, and set `NUM_GEN` to the number of generated sentences you would like
3. For any source text you want to generate sentences from, add an entry to `selected_files` in the format `"SOURCE_TITLE": "path/to/txt/file",`. For the included texts, copy and paste from the `files` dictionary
4. From the home directory, run the command `$ python3 src/text-analysis.py`

# Method
I ended up using the Natural Language Toolkit (nltk) package to do the text processing. My initial approach (scifi-sentence-generation) resembled one from the NLP for Hackers blog, and it used a smaller corpus of science-fiction sentences from the Brown Corpus that comes with nltk to create a trigram model which was used to generate a sentence.

After I got the basic method working, I decided to extend it so it could be used with any source of text and with n-grams of different sizes. I downloaded the text of all of the Harry Potter books as well as "The Omnivore's Dilemma" (a nutrition book) to test the new method on.

To generate sentences from a text, it first loads in the text and does some basic cleaning like removing badly formatted lines or non-ASCII characters. It then creates a Text object from the full text, which breaks it down into sentences, and then creates padded n-grams of a chosen size out of each sentence. The first (n-1) words of each n-gram are used as a key in dictionary, with the value being itself a dictionary mapping the final word appearing in an n-gram to a count of how many times it appears. These counts are converted into probabilities, which are used to generate a sentence by repeatedly taking the last (n-1) words generated (starting with empty padding) and randomly picking one of the words that would follow it according to the probabilities of each occuring. Once a model is created, many sentences can be generated in a row without recreating the model. In addition, the Text object also includes a method for finding the most likely sentence from a given n-gram size for a given text.

Some resulting sentences:

*"He found the page on Doxys in Gilderoy Lockharts Guide to Household Pests, which was lit instead with the right to give evidence if you can beat up a little pink"* (Harry Potter and the Order of the Phoenix)

*"But Voldemort continues to set up several new offices in response."* (Harry Potter and the Half-Blood Prince)

*"The Selection of Foods by Rats, Humans, and yet I realize I'm actually a third-generation alternative farmer, these fleshy buttons of protein bars and shakes."* (The Omnivore's Dilemma)

# Comparing different size ngrams

**n = 2 (bigrams)**
Using just bigrams, the resulting sentences were very original, but not very comprehensible. As each word is picked with reference to only the previous word, there is not much continuity over the course of the sentence.

Examples:
*"Maybe Harry, but they were planning the castle and long, pulled a few days later and pulled his forearm."* (Harry Potter and the Goblet of Fire)

*"In a given day a good or more philosophy and yield potential for giving them to Produce on the bacteria that took a name of local markets with little different from?"* (The Omnivore's Dilemma)

**n = 3 (trigrams)**
A trigram model does a better job at generating sentences that sounds relatively correct, making less obvious grammatical mistakes like changing tenses or including two objects in a sentence. The generated sentences are generally fairly original, except for smaller sentences (like *"Constant vigilance."*) which are often lifted directly from the source.

Examples:
*"But they got to tell your grandchildren, that's what you seek stays here to search my office, then!"* (Harry Potter and the Goblet of Fire)

*"Every meal we were better off leaving the plants and animals that the price of whiskey plummeted to the same man doing both."* (The Omnivore's Dilemma)

**n = 4**
The 4-gram model arguable produces the best generated sentences, as choosing each word based off the previous 3 gives a decent amount of continuity to each part of the sentence and avoids many obvious grammatical mistakes. The sentences largely are still original, mostly not appearing in the source. However, as there are fewer options to follow a given set of 3 preceding words, the generated sentences frequently stitch together a few strings of 6 or so words taken directly from the source.

Examples:
*"'He'd just say Crouch was refusing to come anywhere near him, Professor Karkaroff,' said Moody, his normal eye moving steadily down the list while his magical eye fixed upon his back, a look of deep skepticism behind Dumbledore's back."* (Harry Potter and the Goblet of Fire)

*"Well, food is where and why it began, as the cows and pigs and grass, without getting into specifics about the manure and grubs and composted guts that made the whole dance work."* (The Omnivore's Dilemma)

**n = 5**
The 5-gram model shows an even more extreme version of the stitching together a few sentences, often just reproducing an entire sentence directly from the text. In a 10 sentence sample from *Harry Potter and the Goblet of Fire* and *The Omnivore's Dilemma*, 6 and 8 sentences respectively were exact copies of sentences in the source. As such, the generated sentences are well formed, but generally not very original. If original, they largely are comprised of only 2 or 3 sentence fragments.

Examples:
*"Harry doubted very much if any of the girls who had not been selected had dissolved into tears and were sobbing with their heads on their arms."* (Harry Potter and the Goblet of Fire)

*"My plan had been a cozy winter dinner, but I could see why doing it as fast and as surely as possible was best for all concerned."* (The Omnivore's Dilemma)


# Credits
Trigram model inspiration: [NLP for Hackers Blog](https://nlpforhackers.io/language-models/)

NLTK documentation/tutorial: [nltk book](http://www.nltk.org/book/)


Texts used:
*Harry Potter*, J.K. Rowling [Source](http://www.glozman.com/textpages.html)

*The Omnivore's Dilemma*, Michael Pollan. [Source](https://archive.org/stream/Michael_Pollan-The_Omnivores_Dilemma/Michael_Pollan-The_Omnivores_Dilemma_djvu.txt)
